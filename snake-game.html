<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="Q-Learning implementation for Snake game with 8-bit state encoding, achieving 33.20 average score and 24.2% board coverage" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developed by Orson.io team" name="author">

  <meta name="msapplication-tap-highlight" content="no">

  <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-icon-180x180.png">
  <link href="./assets/favicon.ico" rel="icon">

  <title>Reinforcement Learning Snake Game - Kacper Kowalski</title>
  <link href="./main.d8e0d294.css" rel="stylesheet">
  
  <style>
    .project-img {
      width: 100%;
      max-width: 800px;
      height: auto;
      display: block;
      margin: 15px auto;
      border-radius: 6px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease;
    }
    
    .project-img:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }
    
    .white-card {
      margin-bottom: 30px;
      border-radius: 6px;
    }
    
    .white-card img {
      width: 100%;
      max-width: 800px;
      height: auto;
      margin: 0 auto;
      display: block;
      border-radius: 6px;
    }
    
    .card-block {
      padding: 25px;
    }
    
    h2 {
      margin-top: 0;
      margin-bottom: 25px;
      color: #333;
    }
  </style>
</head>

<body>

  <main class="content-wrapper">
    <header class="white-text-container section-container">
      <div class="text-center">
        <h1 style="color: white; text-align: center;">Reinforcement Learning Snake Game</h1>
      </div>
    </header>

    <div class="card white-card">
      <img src="./assets/images/snake-game/Snake Main.jpg" alt="Reinforcement learning snake game demonstration" class="project-img">
    </div>

    <div class="card white-card">
      <img src="./assets/images/snake-game/snake_gameplay.gif" alt="Snake AI gameplay demonstration showing learned behavior" class="project-img">
      <p style="color: black; text-align: center;">Live gameplay demonstration of the trained Q-Learning agent navigating the board.</p>
    </div>

    <div class="card">
      <div class="card-block">
        <h2 style="color: black; text-align: center;">About the Project</h2>
        
        <p style="text-align: center; margin-bottom: 20px;">
          <a href="https://github.com/Kapi1243/snake-reinforcement-learning" class="btn btn-primary" target="_blank">
            <i class="fa fa-github"></i> View Code on GitHub
          </a>
        </p>

        <p>A reinforcement learning agent that learns to play Snake using tabular Q-Learning in a custom Python environment. The agent improves purely through interaction with the game—no scripted strategy—learning a policy that balances reaching food with avoiding collisions.</p>

        <h4 class="h5" style="margin-top: 25px;">Highlights</h4>
        <ul>
          <li>Custom Snake environment built for RL-style training and evaluation</li>
          <li>Tabular Q-Learning with epsilon-greedy exploration and decay</li>
          <li>Compact state encoding (8-bit binary, 256 discrete states) to keep learning feasible and fast</li>
          <li>Reward design to speed up learning and reduce random wandering early on</li>
          <li>Reproducible experiments with seeded runs and logged results</li>
          <li>Visual outputs: training curves and gameplay GIFs</li>
        </ul>

        <h4 class="h5">How it works</h4>
        <p>At each step, the agent observes a compact representation of the board (food direction + immediate collision risks), selects an action, receives a reward signal, and updates its Q-table via the Bellman update:</p>
        <p style="text-align: center; font-style: italic; margin: 15px 0;">Q(s,a) ← Q(s,a) + α(r + γ max Q(s',a') − Q(s,a))</p>
        <p><strong>Key hyperparameters:</strong> Learning rate α = 0.1, discount factor γ = 0.9, epsilon decay = 0.9995 per episode (0.2 → 0.05)</p>
        <p><strong>Performance:</strong> Best average score of 33.20, highest single score of 62 (24.2% board coverage) with ~15,700 training episodes.</p>

        <h4 class="h5">What I learned</h4>
        <ul>
          <li>MDPs and value functions, exploration vs exploitation trade-offs</li>
          <li>State design to prevent state-space explosion (8-bit encoding reduces 2^32 potential states to just 256)</li>
          <li>Reward shaping for faster convergence—sparse rewards slow learning, so designed multi-tier structure: food eating (+10 + length×0.5), moving toward food (+1.1), survival bonus (+0.1 per step), collision (-10)</li>
          <li>Writing maintainable RL code with type hints, modular structure, and model persistence</li>
        </ul>

        <h4 class="h5">Next steps</h4>
        <ul>
          <li>Compare against DQN-based agents (Double/Dueling DQN) for scalability</li>
          <li>Test with larger boards and richer state representations</li>
          <li>Curriculum learning to gradually increase difficulty</li>
        </ul>

        <h4 class="h5">Tech</h4>
        <p>Python • NumPy • Reinforcement Learning • Q-Learning</p>
      </div>
    </div>

    <div class="card white-card">
      <img src="./assets/images/snake-game/snake_snapshot.png" alt="Q-Learning Snake AI performance snapshot" class="project-img">
      <p style="color: black; text-align: center;">Performance snapshot showing the agent's achievements and key metrics.</p>
      <a href="index.html" class="btn btn-secondary" style="text-align: center;">Back to Projects</a>
    </div>
  </main>

</body>

</html>
