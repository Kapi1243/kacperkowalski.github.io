<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="Q-Learning implementation for Snake game with 8-bit state encoding, achieving 33.20 average score and 24.2% board coverage" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developed by Orson.io team" name="author">

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">

  <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-icon-180x180.png">
  <link href="./assets/favicon.ico" rel="icon">

  <title>Reinforcement Learning Snake Game - Kacper Kowalski</title>
  <link href="./main.d8e0d294.css" rel="stylesheet">
</head>

<body>

  <main class="content-wrapper">
    <header class="white-text-container section-container">
      <div class="text-center">
        <h1 style="color: white; text-align: center;">Reinforcement Learning Snake Game</h1>
      </div>
    </header>

    <div class="card white-card">
      <img src="./assets/images/img-02.jpg" alt="Reinforcement learning snake game demonstration" class="project-img">
    </div>

    <div class="card">
      <div class="card-block">
        <h2 style="color: black; text-align: center;">About the Project</h2>
        <h4 class="h5">Technologies: Python, Q-Learning, Reinforcement Learning, NumPy</h4>
        
        <p style="text-align: center; margin-bottom: 20px;">
          <a href="https://github.com/Kapi1243/snake-reinforcement-learning" class="btn btn-primary" target="_blank">
            <i class="fa fa-github"></i> View Code on GitHub
          </a>
        </p>

        <h4 class="h5">üéØ Project Overview:</h4>
        <p>A modern implementation of Q-Learning applied to the classic Snake game. This project demonstrates fundamental reinforcement learning concepts using a custom-built environment and tabular Q-learning agent. The agent learns to play Snake through reinforcement learning without any hardcoded game strategy, developing an optimal policy purely from experience using rewards and penalties.</p>

        <h4 class="h5">üìä Performance Metrics:</h4>
        <ul>
          <li><strong>Best Average Score:</strong> 33.20 over evaluation episodes</li>
          <li><strong>Best Single Score:</strong> 62 (24.2% board coverage on 16√ó16 grid)</li>
          <li><strong>Training Episodes:</strong> ~15,700 (with early stopping)</li>
          <li><strong>State Space:</strong> 256 discrete states (8-bit binary encoding)</li>
          <li><strong>Board Size:</strong> 16√ó16 grid (256 squares total)</li>
        </ul>

        <h4 class="h5">‚ú® Key Features:</h4>
        <ul>
          <li><strong>Custom Snake Environment:</strong> Grid-based game implementation optimized for RL</li>
          <li><strong>Tabular Q-Learning:</strong> Incremental updates with epsilon decay (Œµ: 0.2 ‚Üí 0.05)</li>
          <li><strong>Efficient State Representation:</strong> 8-dimensional binary encoding
            <ul>
              <li>Bits 0-3: Obstacle detection (‚Üë ‚Üí ‚Üì ‚Üê)</li>
              <li>Bits 4-7: Food direction (‚Üë ‚Üí ‚Üì ‚Üê)</li>
            </ul>
          </li>
          <li><strong>Length-Based Rewards:</strong> Scaled rewards encouraging longer survival (+10.0 + length √ó 0.5 for food)</li>
          <li><strong>Early Stopping:</strong> Automatic training termination when performance plateaus</li>
          <li><strong>Training Visualization:</strong> Auto-generated performance curves and convergence graphs</li>
          <li><strong>Model Persistence:</strong> Save and load trained Q-tables</li>
          <li><strong>Type Hints & Documentation:</strong> Production-quality code standards</li>
        </ul>

        <h4 class="h5">üß† How It Works:</h4>
        <p><strong>Q-Learning Algorithm:</strong></p>
        <p>The agent uses the incremental Q-learning update rule: Q(s, a) ‚Üê Q(s, a) + Œ± ¬∑ [r + Œ≥ ¬∑ max[Q(s', a')] - Q(s, a)]</p>
        <ul>
          <li><strong>Learning Rate (Œ±):</strong> 0.1</li>
          <li><strong>Discount Factor (Œ≥):</strong> 0.9</li>
          <li><strong>Epsilon Decay:</strong> 0.9995 per episode</li>
        </ul>

        <p><strong>Reward Structure:</strong></p>
        <ul>
          <li>Eating food: +10.0 + (length √ó 0.5)</li>
          <li>Moving toward food: +1.1</li>
          <li>Survival (each step): +0.1</li>
          <li>Collision (death): -10</li>
        </ul>

        <h4 class="h5">üîß Technical Implementation:</h4>
        <ul>
          <li><strong>State Encoding:</strong> Compact 8-bit binary representation allows 2^8 = 256 states, making tabular Q-learning feasible</li>
          <li><strong>Action Space:</strong> 4 discrete actions (up, right, down, left)</li>
          <li><strong>Environment Design:</strong> Custom Snake.py with Markov Decision Process (MDP) structure</li>
          <li><strong>Training Pipeline:</strong> QLearningAgent.py with configurable hyperparameters</li>
          <li><strong>Visualization Tools:</strong> Performance curves, Q-value convergence plots, animated training GIFs</li>
        </ul>

        <h4 class="h5">üöÄ Usage Examples:</h4>
        <p><strong>Train a new agent:</strong></p>
        <code style="background: #f4f4f4; padding: 10px; display: block; margin: 10px 0;">python src/QLearningAgent.py</code>
        
        <p><strong>Use pre-trained model:</strong></p>
        <code style="background: #f4f4f4; padding: 10px; display: block; margin: 10px 0;">
from src.QLearningAgent import QLearningAgent<br>
agent = QLearningAgent()<br>
agent.load('models/q_learning_snake.pkl')<br>
avg_score, scores = agent.evaluate(board_size=16, num_episodes=100)
        </code>

        <h4 class="h5">üí° Challenges & Solutions:</h4>
        <ul>
          <li><strong>State Space Explosion:</strong> A naive state representation would result in millions of states. Solved by designing an efficient 8-bit binary encoding that captures only essential information (obstacle detection + food direction), reducing to just 256 states.</li>
          <li><strong>Sparse Rewards:</strong> Early training showed slow learning due to infrequent food encounters. Solved by implementing a multi-tier reward structure: small positive reward (+1.1) for moving toward food, survival bonus (+0.1 per step), and length-scaled food reward (+10.0 + length √ó 0.5).</li>
          <li><strong>Exploration vs. Exploitation:</strong> Agent got stuck in local optima with fixed epsilon. Solved by implementing epsilon decay (0.9995 per episode) from 0.2 to 0.05, allowing high initial exploration that gradually shifts to exploitation of learned policy.</li>
          <li><strong>Training Efficiency:</strong> Training took excessive time without convergence detection. Solved by implementing early stopping mechanism that monitors average score plateau over 500 episodes, reducing training from potential 20,000+ to ~15,700 episodes.</li>
        </ul>

        <h4 class="h5">üéì Learning Outcomes:</h4>
        <ul>
          <li>Markov Decision Processes (MDPs) and state-action value functions</li>
          <li>Q-learning algorithm with temporal difference updates</li>
          <li>Exploration-exploitation trade-off via epsilon-greedy strategy</li>
          <li>State space engineering and feature representation</li>
          <li>Reward shaping for faster convergence</li>
          <li>Object-oriented Python design with type hints</li>
          <li>Model persistence and evaluation techniques</li>
        </ul>

        <h4 class="h5">üîÆ Future Improvements:</h4>
        <ul>
          <li>Deep Q-Networks (DQN) to handle larger state spaces</li>
          <li>Double DQN to reduce overestimation bias</li>
          <li>Dueling DQN with separate value and advantage streams</li>
          <li>Prioritized Experience Replay for efficient learning</li>
          <li>Multi-step TD learning (n-step returns)</li>
          <li>Curriculum learning with gradually increasing board size</li>
        </ul>
      </div>
    </div>

    <div class="card white-card">
      <img src="./assets/images/snakegame.png" alt="Q-Learning Snake AI training progress showing performance curves and convergence" class="project-img">
      <p style="color: black; text-align: center;">Training progress showing the agent's learning curve over ~15,700 episodes with early stopping.</p>
      <a href="index.html" class="btn btn-secondary" style="text-align: center;">Back to Projects</a>
    </div>
  </main>

</body>

</html>
